{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luhg/anaconda3/envs/LightRAG/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from lightrag import LightRAG, QueryParam\n",
    "from lightrag.llm import openai_complete_if_cache\n",
    "from lightrag.llm import ollama_embedding\n",
    "from lightrag.utils import EmbeddingFunc\n",
    "from dotenv import load_dotenv\n",
    "import nest_asyncio \n",
    "nest_asyncio.apply() \n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKING_DIR = \"../data/test_neo4j\"\n",
    "\n",
    "if not os.path.exists(WORKING_DIR):\n",
    "    os.mkdir(WORKING_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "import time\n",
    "\n",
    "def rate_counter(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        if not hasattr(wrapper, 'call_count'):\n",
    "            wrapper.call_count = 0\n",
    "            wrapper.start_time = time.time()\n",
    "        \n",
    "        wrapper.call_count += 1\n",
    "        wrapper.end_time = time.time()\n",
    "        elapsed_time = wrapper.end_time - wrapper.start_time\n",
    "        wrapper.rate = wrapper.call_count / (elapsed_time / 60)  # 次/分钟\n",
    "\n",
    "        return func(*args, **kwargs)\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@rate_counter\n",
    "async def test():\n",
    "    print(\"1\", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "函数调用次数: 100，总时间: 0.00021004676818847656s，速率: 28565066.96935301 次/分钟\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "# for i in range(100):\n",
    "#     await test()\n",
    "#     await asyncio.sleep(0.02)\n",
    "test.start_time = time.time()\n",
    "asyncio.gather(*[test() for i in range(100)])\n",
    "    \n",
    "print()\n",
    "print(f\"函数调用次数: {test.call_count}，总时间: {test.end_time - test.start_time}s，速率: {test.rate} 次/分钟\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111"
     ]
    }
   ],
   "source": [
    "@rate_counter\n",
    "async def llm_model_func(\n",
    "    prompt, system_prompt=None, history_messages=[], **kwargs\n",
    ") -> str:\n",
    "    return await openai_complete_if_cache(\n",
    "        \"qwen-plus\",\n",
    "        prompt,\n",
    "        system_prompt=system_prompt,\n",
    "        history_messages=history_messages,\n",
    "        api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "        base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "        **kwargs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models.tongyi import ChatTongyi\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "llm = ChatTongyi(model=\"qwen-plus\")\n",
    "\n",
    "@rate_counter\n",
    "@retry(\n",
    "    stop=stop_after_attempt(5),\n",
    "    wait=wait_exponential(multiplier=1, min=4, max=20),\n",
    ")\n",
    "async def llm_model_func(\n",
    "    prompt, system_prompt=None, history_messages=[], **kwargs\n",
    ") -> str:\n",
    "    messages = []\n",
    "    if system_prompt:\n",
    "        messages.append(SystemMessage(content=system_prompt))\n",
    "    if history_messages:\n",
    "        # for msg in history_messages:\n",
    "        #     messages.append(HumanMessage(content=msg[\"content\"]) if msg[\"role\"] == \"user\" else AIMessage(content=msg[\"content\"]))\n",
    "        messages.extend([HumanMessage(content=msg[\"content\"]) if msg[\"role\"] == \"user\" else AIMessage(content=msg[\"content\"]) for msg in history_messages])\n",
    "    messages.append(HumanMessage(content=prompt))\n",
    "    \n",
    "    msg = await llm.ainvoke(messages)\n",
    "    \n",
    "    return msg.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightRAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightrag:Logger initialized for working directory: ../data/test_neo4j\n",
      "DEBUG:lightrag:LightRAG init with param:\n",
      "  working_dir = ../data/test_neo4j,\n",
      "  workspace = test_lightrag,\n",
      "  kv_storage = JsonKVStorage,\n",
      "  vector_storage = PostgresVectorDBStorage,\n",
      "  graph_storage = Neo4JStorage,\n",
      "  log_level = DEBUG,\n",
      "  chunk_token_size = 500,\n",
      "  chunk_overlap_token_size = 100,\n",
      "  tiktoken_model_name = gpt-4o-mini,\n",
      "  entity_extract_max_gleaning = 1,\n",
      "  entity_summary_to_max_tokens = 500,\n",
      "  node_embedding_algorithm = node2vec,\n",
      "  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},\n",
      "  embedding_func = {'embedding_dim': 1024, 'max_token_size': 512, 'func': <function <lambda> at 0x7fe9fa168b80>},\n",
      "  embedding_batch_num = 32,\n",
      "  embedding_func_max_async = 16,\n",
      "  llm_model_func = <function llm_model_func at 0x7fe9faa031a0>,\n",
      "  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,\n",
      "  llm_model_max_token_size = 32768,\n",
      "  llm_model_max_async = 16,\n",
      "  llm_model_kwargs = {},\n",
      "  vector_db_storage_cls_kwargs = {},\n",
      "  enable_llm_cache = True,\n",
      "  addon_params = {},\n",
      "  convert_response_to_json_func = <function convert_response_to_json at 0x7fea012f7920>\n",
      "\n",
      "INFO:lightrag:Load KV llm_response_cache with 49 data\n",
      "INFO:lightrag:Load KV full_docs with 7 data\n",
      "INFO:lightrag:Load KV text_chunks with 36 data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightrag:Tables created successfully\n",
      "INFO:lightrag:Tables created successfully\n",
      "INFO:lightrag:Tables created successfully\n"
     ]
    }
   ],
   "source": [
    "rag = LightRAG(\n",
    "    working_dir=WORKING_DIR,\n",
    "    workspace=\"test_postgres_storage\",\n",
    "    llm_model_func=llm_model_func,\n",
    "    embedding_func=EmbeddingFunc(\n",
    "        embedding_dim=1024,\n",
    "        max_token_size=512,\n",
    "        func=lambda texts: ollama_embedding(\n",
    "            texts, embed_model=\"viosay/conan-embedding-v1:latest\", host=\"http://192.168.69.234:11343\"\n",
    "        ),\n",
    "    ),\n",
    "    chunk_token_size=500,\n",
    "    log_level=\"DEBUG\",\n",
    "    vector_storage=\"PostgresVectorDBStorage\",\n",
    "    graph_storage=\"Neo4JStorage\"\n",
    ")\n",
    "\n",
    "# rag.vector_db_storage_cls.db = postgresql_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['title: 基于WWW的交互式网络课件系统的开发技术\\nauthors: 傅秀芬，汤庸\\nsource: 计算机工程与应用\\nsourceDetail: \\ndate: 1998.-\\ntype: 期刊论文\\nkeyword: \\nsummary: ',\n",
       " 'title: 时态变量“Now”语义及相应时态关系运算\\nauthors: 叶小平，汤庸\\nsource: 软件学报，2005，16（5）：838-845\\nsourceDetail: \\ndate: 2005.05\\ntype: 期刊论文\\nkeyword: \\nsummary: ',\n",
       " 'title: 计算机支持的协同工作概观\\nauthors: 汤庸\\nsource: 工业工程,1999,2(003):10-12\\nsourceDetail: \\ndate: 1999.01\\ntype: 期刊论文\\nkeyword: \\nsummary: ',\n",
       " 'title: 时态知识和时态数据的统一模型研究\\nauthors: 汤庸，汤娜，叶小平，冯智圣，肖炜\\nsource: 软件学报，2003，14(S),74-79 【EI】\\nsourceDetail: \\ndate: 2003.11\\ntype: 期刊论文\\nkeyword: \\nsummary: ',\n",
       " 'title: 基于描述逻辑的CIM模型\\nauthors: 蒋运承 汤庸 王驹 周生明\\nsource: 微电子学与计算机,2007,24(012):55-58\\nsourceDetail: \\ndate: 2007.-\\ntype: 期刊论文\\nkeyword: \\nsummary: ',\n",
       " 'title: CD—ROM信息存储与检索技术\\nauthors: 汤庸\\nsource: 计算机时代     1996年 第01期\\nsourceDetail: \\ndate: 1996.01\\ntype: 期刊论文\\nkeyword: \\nsummary: ']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "doc_path = \"../data/paper/scholat_paper_ed/scholat_paper_ed_001.csv\"\n",
    "\n",
    "loader = CSVLoader(doc_path)\n",
    "data = loader.load()\n",
    "\n",
    "data = [d.page_content for d in data]\n",
    "need_to_insert_data = data[4:10]\n",
    "need_to_insert_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EntityStorage(namespace='entities', global_config={'working_dir': '../data/test_neo4j', 'workspace': 'test_lightrag', 'kv_storage': 'JsonKVStorage', 'vector_storage': 'PostgresVectorDBStorage', 'graph_storage': 'Neo4JStorage', 'log_level': 'DEBUG', 'chunk_token_size': 500, 'chunk_overlap_token_size': 100, 'tiktoken_model_name': 'gpt-4o-mini', 'entity_extract_max_gleaning': 1, 'entity_summary_to_max_tokens': 500, 'node_embedding_algorithm': 'node2vec', 'node2vec_params': {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3}, 'embedding_func': {'embedding_dim': 1024, 'max_token_size': 512, 'func': <function <lambda> at 0x7fe9fa168b80>}, 'embedding_batch_num': 32, 'embedding_func_max_async': 16, 'llm_model_func': <function llm_model_func at 0x7fe9faa031a0>, 'llm_model_name': 'meta-llama/Llama-3.2-1B-Instruct', 'llm_model_max_token_size': 32768, 'llm_model_max_async': 16, 'llm_model_kwargs': {}, 'vector_db_storage_cls_kwargs': {}, 'enable_llm_cache': True, 'addon_params': {}, 'convert_response_to_json_func': <function convert_response_to_json at 0x7fea012f7920>}, embedding_func=EmbeddingFunc(embedding_dim=1024, max_token_size=512, func=<function <lambda> at 0x7fe9fa168b80>), meta_fields=set())\n",
      "RelationshipStorage(namespace='relationships', global_config={'working_dir': '../data/test_neo4j', 'workspace': 'test_lightrag', 'kv_storage': 'JsonKVStorage', 'vector_storage': 'PostgresVectorDBStorage', 'graph_storage': 'Neo4JStorage', 'log_level': 'DEBUG', 'chunk_token_size': 500, 'chunk_overlap_token_size': 100, 'tiktoken_model_name': 'gpt-4o-mini', 'entity_extract_max_gleaning': 1, 'entity_summary_to_max_tokens': 500, 'node_embedding_algorithm': 'node2vec', 'node2vec_params': {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3}, 'embedding_func': {'embedding_dim': 1024, 'max_token_size': 512, 'func': <function <lambda> at 0x7fe9fa168b80>}, 'embedding_batch_num': 32, 'embedding_func_max_async': 16, 'llm_model_func': <function llm_model_func at 0x7fe9faa031a0>, 'llm_model_name': 'meta-llama/Llama-3.2-1B-Instruct', 'llm_model_max_token_size': 32768, 'llm_model_max_async': 16, 'llm_model_kwargs': {}, 'vector_db_storage_cls_kwargs': {}, 'enable_llm_cache': True, 'addon_params': {}, 'convert_response_to_json_func': <function convert_response_to_json at 0x7fea012f7920>}, embedding_func=EmbeddingFunc(embedding_dim=1024, max_token_size=512, func=<function <lambda> at 0x7fe9fa168b80>), meta_fields=set())\n",
      "ChunkStorage(namespace='chunks', global_config={'working_dir': '../data/test_neo4j', 'workspace': 'test_lightrag', 'kv_storage': 'JsonKVStorage', 'vector_storage': 'PostgresVectorDBStorage', 'graph_storage': 'Neo4JStorage', 'log_level': 'DEBUG', 'chunk_token_size': 500, 'chunk_overlap_token_size': 100, 'tiktoken_model_name': 'gpt-4o-mini', 'entity_extract_max_gleaning': 1, 'entity_summary_to_max_tokens': 500, 'node_embedding_algorithm': 'node2vec', 'node2vec_params': {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3}, 'embedding_func': {'embedding_dim': 1024, 'max_token_size': 512, 'func': <function <lambda> at 0x7fe9fa168b80>}, 'embedding_batch_num': 32, 'embedding_func_max_async': 16, 'llm_model_func': <function llm_model_func at 0x7fe9faa031a0>, 'llm_model_name': 'meta-llama/Llama-3.2-1B-Instruct', 'llm_model_max_token_size': 32768, 'llm_model_max_async': 16, 'llm_model_kwargs': {}, 'vector_db_storage_cls_kwargs': {}, 'enable_llm_cache': True, 'addon_params': {}, 'convert_response_to_json_func': <function convert_response_to_json at 0x7fea012f7920>}, embedding_func=EmbeddingFunc(embedding_dim=1024, max_token_size=512, func=<function <lambda> at 0x7fe9fa168b80>), meta_fields=set())\n"
     ]
    }
   ],
   "source": [
    "print(rag.entities_vdb)\n",
    "print(rag.relationships_vdb)\n",
    "print(rag.chunks_vdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function PostgresStorageFactory.get_storage_class at 0x7fe9fc0a49a0>\n"
     ]
    }
   ],
   "source": [
    "print(rag.vector_storage_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql+asyncpg://postgres:***@localhost:6024/test_postgres\n"
     ]
    }
   ],
   "source": [
    "print(rag.entities_vdb.storage.engine.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# await rag.entities_vdb.storage.init_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await rag.ainsert(need_to_insert_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the content of the file\n",
    "with open(\"../data/temp.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    file_content = file.read()\n",
    "\n",
    "llm_model_func.start_time = time.time()\n",
    "# await rag.ainsert(file_content)\n",
    "# print(f\"函数调用次数: {llm_model_func.call_count}，总时间: {llm_model_func.end_time - llm_model_func.start_time}s，速率: {llm_model_func.rate} 次/分钟\")\n",
    "\n",
    "tasks = []\n",
    "tasks.append(rag.ainsert(file_content))\n",
    "\n",
    "async def print_rate():\n",
    "    await asyncio.sleep(60)\n",
    "    for i in range(18):\n",
    "        await asyncio.sleep(10)\n",
    "        print(f\"函数调用次数: {llm_model_func.call_count}，总时间: {llm_model_func.end_time - llm_model_func.start_time}s，速率: {llm_model_func.rate} 次/分钟\")\n",
    "\n",
    "tasks.append(print_rate())\n",
    "\n",
    "await asyncio.gather(*tasks)\n",
    "print(f\"函数调用次数: {llm_model_func.call_count}，总时间: {llm_model_func.end_time - llm_model_func.start_time}s，速率: {llm_model_func.rate} 次/分钟\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# await rag.adelete_by_entity(\"汤庸\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = await rag.aquery(\n",
    "        \"汤庸有哪些成就？\",\n",
    "        param=QueryParam(mode=\"local\", only_need_context=True),\n",
    "    )\n",
    "\n",
    "print(len(res))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可视化本地图为网页"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "import random\n",
    "\n",
    "# Load the GraphML file\n",
    "G = nx.read_graphml(\"../data/test_paper/graph_chunk_entity_relation.graphml\")\n",
    "\n",
    "# Create a Pyvis network\n",
    "net = Network(height=\"100vh\", notebook=True)\n",
    "\n",
    "# Convert NetworkX graph to Pyvis network\n",
    "net.from_nx(G)\n",
    "\n",
    "# Add colors to nodes\n",
    "for node in net.nodes:\n",
    "    node[\"color\"] = \"#{:06x}\".format(random.randint(0, 0xFFFFFF))\n",
    "\n",
    "# Save and display the network\n",
    "net.show(\"../data/test_paper/knowledge_graph.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可视化本地图到 neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from lightrag.utils import xml_to_json\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Constants\n",
    "BATCH_SIZE_NODES = 500\n",
    "BATCH_SIZE_EDGES = 100\n",
    "\n",
    "# Neo4j connection credentials\n",
    "NEO4J_URI = \"bolt://localhost:7687\"\n",
    "NEO4J_USERNAME = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"12345678\"\n",
    "\n",
    "\n",
    "def convert_xml_to_json(xml_path, output_path):\n",
    "    \"\"\"Converts XML file to JSON and saves the output.\"\"\"\n",
    "    if not os.path.exists(xml_path):\n",
    "        print(f\"Error: File not found - {xml_path}\")\n",
    "        return None\n",
    "\n",
    "    json_data = xml_to_json(xml_path)\n",
    "    if json_data:\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(json_data, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"JSON file created: {output_path}\")\n",
    "        return json_data\n",
    "    else:\n",
    "        print(\"Failed to create JSON data\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_in_batches(tx, query, data, batch_size):\n",
    "    \"\"\"Process data in batches and execute the given query.\"\"\"\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i : i + batch_size]\n",
    "        tx.run(query, {\"nodes\": batch} if \"nodes\" in query else {\"edges\": batch})\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Paths\n",
    "    xml_file = os.path.join(WORKING_DIR, \"graph_chunk_entity_relation.graphml\")\n",
    "    json_file = os.path.join(WORKING_DIR, \"graph_data.json\")\n",
    "\n",
    "    # Convert XML to JSON\n",
    "    json_data = convert_xml_to_json(xml_file, json_file)\n",
    "    if json_data is None:\n",
    "        return\n",
    "\n",
    "    # Load nodes and edges\n",
    "    nodes = json_data.get(\"nodes\", [])\n",
    "    edges = json_data.get(\"edges\", [])\n",
    "\n",
    "    # Neo4j queries\n",
    "    create_nodes_query = \"\"\"\n",
    "    UNWIND $nodes AS node\n",
    "    MERGE (e:Entity {id: node.id})\n",
    "    SET e.entity_type = node.entity_type,\n",
    "        e.description = node.description,\n",
    "        e.source_id = node.source_id,\n",
    "        e.displayName = node.id\n",
    "    REMOVE e:Entity\n",
    "    WITH e, node\n",
    "    CALL apoc.create.addLabels(e, [node.entity_type]) YIELD node AS labeledNode\n",
    "    RETURN count(*)\n",
    "    \"\"\"\n",
    "\n",
    "    create_edges_query = \"\"\"\n",
    "    UNWIND $edges AS edge\n",
    "    MATCH (source {id: edge.source})\n",
    "    MATCH (target {id: edge.target})\n",
    "    WITH source, target, edge,\n",
    "         CASE\n",
    "            WHEN edge.keywords CONTAINS 'lead' THEN 'lead'\n",
    "            WHEN edge.keywords CONTAINS 'participate' THEN 'participate'\n",
    "            WHEN edge.keywords CONTAINS 'uses' THEN 'uses'\n",
    "            WHEN edge.keywords CONTAINS 'located' THEN 'located'\n",
    "            WHEN edge.keywords CONTAINS 'occurs' THEN 'occurs'\n",
    "           ELSE REPLACE(SPLIT(edge.keywords, ',')[0], '\\\"', '')\n",
    "         END AS relType\n",
    "    CALL apoc.create.relationship(source, relType, {\n",
    "      weight: edge.weight,\n",
    "      description: edge.description,\n",
    "      keywords: edge.keywords,\n",
    "      source_id: edge.source_id\n",
    "    }, target) YIELD rel\n",
    "    RETURN count(*)\n",
    "    \"\"\"\n",
    "\n",
    "    set_displayname_and_labels_query = \"\"\"\n",
    "    MATCH (n)\n",
    "    SET n.displayName = n.id\n",
    "    WITH n\n",
    "    CALL apoc.create.setLabels(n, [n.entity_type]) YIELD node\n",
    "    RETURN count(*)\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a Neo4j driver\n",
    "    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "\n",
    "    try:\n",
    "        # Execute queries in batches\n",
    "        with driver.session() as session:\n",
    "            # Insert nodes in batches\n",
    "            session.execute_write(\n",
    "                process_in_batches, create_nodes_query, nodes, BATCH_SIZE_NODES\n",
    "            )\n",
    "\n",
    "            # Insert edges in batches\n",
    "            session.execute_write(\n",
    "                process_in_batches, create_edges_query, edges, BATCH_SIZE_EDGES\n",
    "            )\n",
    "\n",
    "            # Set displayName and labels\n",
    "            session.run(set_displayname_and_labels_query)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "\n",
    "    finally:\n",
    "        driver.close()\n",
    "\n",
    "\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
