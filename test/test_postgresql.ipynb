{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luhg/anaconda3/envs/LightRAG/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from lightrag import LightRAG, QueryParam\n",
    "from lightrag.llm import openai_complete_if_cache, openai_embedding\n",
    "from lightrag.llm import ollama_model_complete, ollama_embedding\n",
    "from lightrag.utils import EmbeddingFunc\n",
    "from lightrag.kg.postgresql_impl import PostgresDB\n",
    "import numpy as np\n",
    "import textract\n",
    "from dotenv import load_dotenv\n",
    "import nest_asyncio \n",
    "nest_asyncio.apply() \n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKING_DIR = \"../data/test_postgres\"\n",
    "\n",
    "if not os.path.exists(WORKING_DIR):\n",
    "    os.mkdir(WORKING_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def llm_model_func(\n",
    "    prompt, system_prompt=None, history_messages=[], **kwargs\n",
    ") -> str:\n",
    "    return await openai_complete_if_cache(\n",
    "        \"qwen-plus\",\n",
    "        prompt,\n",
    "        system_prompt=system_prompt,\n",
    "        history_messages=history_messages,\n",
    "        api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "        base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "        **kwargs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "postgresql_db = PostgresDB(\n",
    "    config={\n",
    "        \"host\": \"localhost\",\n",
    "        \"port\": 6024,\n",
    "        \"database\": \"test_postgres\",\n",
    "        \"user\": \"postgres\",\n",
    "        \"password\": \"123456\",\n",
    "        \"workspace\": \"test\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "print(postgresql_db.workspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightrag:Logger initialized for working directory: ../data/test_postgres\n",
      "DEBUG:lightrag:LightRAG init with param:\n",
      "  working_dir = ../data/test_postgres,\n",
      "  kv_storage = JsonKVStorage,\n",
      "  vector_storage = PostgresVectorDBStorage,\n",
      "  graph_storage = NetworkXStorage,\n",
      "  log_level = DEBUG,\n",
      "  chunk_token_size = 500,\n",
      "  chunk_overlap_token_size = 100,\n",
      "  tiktoken_model_name = gpt-4o-mini,\n",
      "  entity_extract_max_gleaning = 1,\n",
      "  entity_summary_to_max_tokens = 500,\n",
      "  node_embedding_algorithm = node2vec,\n",
      "  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},\n",
      "  embedding_func = {'embedding_dim': 1024, 'max_token_size': 512, 'func': <function <lambda> at 0x7f303eab72e0>},\n",
      "  embedding_batch_num = 32,\n",
      "  embedding_func_max_async = 16,\n",
      "  llm_model_func = <function llm_model_func at 0x7f303eab7380>,\n",
      "  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,\n",
      "  llm_model_max_token_size = 32768,\n",
      "  llm_model_max_async = 16,\n",
      "  llm_model_kwargs = {},\n",
      "  vector_db_storage_cls_kwargs = {},\n",
      "  enable_llm_cache = True,\n",
      "  addon_params = {},\n",
      "  convert_response_to_json_func = <function convert_response_to_json at 0x7f30456225c0>\n",
      "\n",
      "INFO:lightrag:Load KV llm_response_cache with 0 data\n",
      "INFO:lightrag:Load KV full_docs with 0 data\n",
      "INFO:lightrag:Load KV text_chunks with 0 data\n",
      "INFO:lightrag:Loaded graph from ../data/test_postgres/graph_chunk_entity_relation.graphml with 0 nodes, 0 edges\n"
     ]
    }
   ],
   "source": [
    "rag = LightRAG(\n",
    "    working_dir=WORKING_DIR,\n",
    "    # kg=\"Neo4JStorage\",\n",
    "    llm_model_func=llm_model_func,\n",
    "    embedding_func=EmbeddingFunc(\n",
    "        embedding_dim=1024,\n",
    "        max_token_size=512,\n",
    "        func=lambda texts: ollama_embedding(\n",
    "            texts, embed_model=\"viosay/conan-embedding-v1:latest\", host=\"http://192.168.69.234:11343\"\n",
    "        ),\n",
    "    ),\n",
    "    chunk_token_size=500,\n",
    "    log_level=\"DEBUG\",\n",
    "    vector_storage=\"PostgresVectorDBStorage\"\n",
    ")\n",
    "\n",
    "rag.vector_db_storage_cls.db = postgresql_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "print(rag.vector_db_storage_cls.db.workspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['title: 基于时限的角色访问控制委托模型\\nauthors: 道炜 汤庸 冀高峰 杨虹轶\\nsource: 计算机科学,2008,35（3）:175-177\\nsourceDetail: \\ndate: 2008.03\\ntype: 期刊论文\\nkeyword: \\nsummary: ']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "doc_path = \"../data/paper/scholat_paper_ed/scholat_paper_ed_001.csv\"\n",
    "\n",
    "loader = CSVLoader(doc_path)\n",
    "data = loader.load()\n",
    "\n",
    "data = [d.page_content for d in data]\n",
    "need_to_insert_data = data[1:2]\n",
    "need_to_insert_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightrag:Connected to PostgreSQL at localhost:6024\n"
     ]
    }
   ],
   "source": [
    "rag.entities_vdb.db = postgresql_db\n",
    "rag.relationships_vdb.db = postgresql_db\n",
    "rag.chunks_vdb.db = postgresql_db\n",
    "\n",
    "\n",
    "await rag.entities_vdb.init_table()\n",
    "await rag.relationships_vdb.init_table()\n",
    "await rag.chunks_vdb.init_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightrag:[New Docs] inserting 1 docs\n",
      "INFO:lightrag:[New Chunks] inserting 1 chunks\n",
      "DEBUG:lightrag:[upserting chunks] {'chunk-64e1449282fc37680aa7d6f2e4a9d92a': {'tokens': 76, 'content': 'title: 基于时限的角色访问控制委托模型\\nauthors: 道炜 汤庸 冀高峰 杨虹轶\\nsource: 计算机科学,2008,35（3）:175-177\\nsourceDetail: \\ndate: 2008.03\\ntype: 期刊论文\\nkeyword: \\nsummary:', 'chunk_order_index': 0, 'full_doc_id': 'doc-64e1449282fc37680aa7d6f2e4a9d92a'}}\n",
      "INFO:httpx:HTTP Request: POST http://192.168.69.234:11343/api/embed \"HTTP/1.1 200 OK\"\n",
      "ERROR:lightrag:PostgreSQL error: invalid input for query argument $4: 76 (expected str, got int)\n",
      "Query: \n",
      "        INSERT INTO light_chunks (id,embedding,workspace,tokens,content,full_doc_id,chunk_order_index)\n",
      "        VALUES ($1,$2,$3,$4,$5,$6,$7)\n",
      "        ON CONFLICT (id) DO UPDATE SET\n",
      "        embedding=excluded.embedding,workspace=excluded.workspace,tokens=excluded.tokens,content=excluded.content,full_doc_id=excluded.full_doc_id,chunk_order_index=excluded.chunk_order_index;\n",
      "        \n",
      "ERROR:lightrag:Error while inserting: invalid input for query argument $4: 76 (expected str, got int)\n",
      "INFO:lightrag:Writing graph with 0 nodes, 0 edges\n"
     ]
    }
   ],
   "source": [
    "await rag.ainsert(need_to_insert_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可视化本地图为网页"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "import random\n",
    "\n",
    "# Load the GraphML file\n",
    "G = nx.read_graphml(\"../data/test_paper/graph_chunk_entity_relation.graphml\")\n",
    "\n",
    "# Create a Pyvis network\n",
    "net = Network(height=\"100vh\", notebook=True)\n",
    "\n",
    "# Convert NetworkX graph to Pyvis network\n",
    "net.from_nx(G)\n",
    "\n",
    "# Add colors to nodes\n",
    "for node in net.nodes:\n",
    "    node[\"color\"] = \"#{:06x}\".format(random.randint(0, 0xFFFFFF))\n",
    "\n",
    "# Save and display the network\n",
    "net.show(\"../data/test_paper/knowledge_graph.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可视化本地图到 neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from lightrag.utils import xml_to_json\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Constants\n",
    "BATCH_SIZE_NODES = 500\n",
    "BATCH_SIZE_EDGES = 100\n",
    "\n",
    "# Neo4j connection credentials\n",
    "NEO4J_URI = \"bolt://localhost:7687\"\n",
    "NEO4J_USERNAME = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"12345678\"\n",
    "\n",
    "\n",
    "def convert_xml_to_json(xml_path, output_path):\n",
    "    \"\"\"Converts XML file to JSON and saves the output.\"\"\"\n",
    "    if not os.path.exists(xml_path):\n",
    "        print(f\"Error: File not found - {xml_path}\")\n",
    "        return None\n",
    "\n",
    "    json_data = xml_to_json(xml_path)\n",
    "    if json_data:\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(json_data, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"JSON file created: {output_path}\")\n",
    "        return json_data\n",
    "    else:\n",
    "        print(\"Failed to create JSON data\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_in_batches(tx, query, data, batch_size):\n",
    "    \"\"\"Process data in batches and execute the given query.\"\"\"\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i : i + batch_size]\n",
    "        tx.run(query, {\"nodes\": batch} if \"nodes\" in query else {\"edges\": batch})\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Paths\n",
    "    xml_file = os.path.join(WORKING_DIR, \"graph_chunk_entity_relation.graphml\")\n",
    "    json_file = os.path.join(WORKING_DIR, \"graph_data.json\")\n",
    "\n",
    "    # Convert XML to JSON\n",
    "    json_data = convert_xml_to_json(xml_file, json_file)\n",
    "    if json_data is None:\n",
    "        return\n",
    "\n",
    "    # Load nodes and edges\n",
    "    nodes = json_data.get(\"nodes\", [])\n",
    "    edges = json_data.get(\"edges\", [])\n",
    "\n",
    "    # Neo4j queries\n",
    "    create_nodes_query = \"\"\"\n",
    "    UNWIND $nodes AS node\n",
    "    MERGE (e:Entity {id: node.id})\n",
    "    SET e.entity_type = node.entity_type,\n",
    "        e.description = node.description,\n",
    "        e.source_id = node.source_id,\n",
    "        e.displayName = node.id\n",
    "    REMOVE e:Entity\n",
    "    WITH e, node\n",
    "    CALL apoc.create.addLabels(e, [node.entity_type]) YIELD node AS labeledNode\n",
    "    RETURN count(*)\n",
    "    \"\"\"\n",
    "\n",
    "    create_edges_query = \"\"\"\n",
    "    UNWIND $edges AS edge\n",
    "    MATCH (source {id: edge.source})\n",
    "    MATCH (target {id: edge.target})\n",
    "    WITH source, target, edge,\n",
    "         CASE\n",
    "            WHEN edge.keywords CONTAINS 'lead' THEN 'lead'\n",
    "            WHEN edge.keywords CONTAINS 'participate' THEN 'participate'\n",
    "            WHEN edge.keywords CONTAINS 'uses' THEN 'uses'\n",
    "            WHEN edge.keywords CONTAINS 'located' THEN 'located'\n",
    "            WHEN edge.keywords CONTAINS 'occurs' THEN 'occurs'\n",
    "           ELSE REPLACE(SPLIT(edge.keywords, ',')[0], '\\\"', '')\n",
    "         END AS relType\n",
    "    CALL apoc.create.relationship(source, relType, {\n",
    "      weight: edge.weight,\n",
    "      description: edge.description,\n",
    "      keywords: edge.keywords,\n",
    "      source_id: edge.source_id\n",
    "    }, target) YIELD rel\n",
    "    RETURN count(*)\n",
    "    \"\"\"\n",
    "\n",
    "    set_displayname_and_labels_query = \"\"\"\n",
    "    MATCH (n)\n",
    "    SET n.displayName = n.id\n",
    "    WITH n\n",
    "    CALL apoc.create.setLabels(n, [n.entity_type]) YIELD node\n",
    "    RETURN count(*)\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a Neo4j driver\n",
    "    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "\n",
    "    try:\n",
    "        # Execute queries in batches\n",
    "        with driver.session() as session:\n",
    "            # Insert nodes in batches\n",
    "            session.execute_write(\n",
    "                process_in_batches, create_nodes_query, nodes, BATCH_SIZE_NODES\n",
    "            )\n",
    "\n",
    "            # Insert edges in batches\n",
    "            session.execute_write(\n",
    "                process_in_batches, create_edges_query, edges, BATCH_SIZE_EDGES\n",
    "            )\n",
    "\n",
    "            # Set displayName and labels\n",
    "            session.run(set_displayname_and_labels_query)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "\n",
    "    finally:\n",
    "        driver.close()\n",
    "\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    await rag.aquery(\n",
    "        \"汤庸发表过哪些论文？\",\n",
    "        param=QueryParam(mode=\"hybrid\"),\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
